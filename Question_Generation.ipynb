{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Question Generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP5IoMQRthpIZdZ858+rFv/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HimathR/Zentern-Public/blob/main/Question_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üöß **Installations**\n",
        "Welcome to the question generator. To start, please follow the instructions as listed here. \n",
        "Run the following code block before each run-time to acquire all necessary libaries and resources needed for executing the program!\n",
        "Please note that full installation can take on average 5-10 minutes, please be patient!"
      ],
      "metadata": {
        "id": "1lUOlnSMZ0yo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqnND7HvZvPi",
        "outputId": "1b2468fc-b03c-4487-91d3-6bc00f432e13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Installations Complete\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/ramsrigouthamg/Questgen.ai &> /dev/null\n",
        "!pip install --quiet git+https://github.com/boudinfl/pke.git &> /dev/null\n",
        "!python -m nltk.downloader universal_tagset &> /dev/null\n",
        "!python -m spacy download en &> /dev/null\n",
        "!wget https://github.com/explosion/sense2vec/releases/download/v1.0.0/s2v_reddit_2015_md.tar.gz &> /dev/null\n",
        "!tar -xvf  s2v_reddit_2015_md.tar.gz &> /dev/null\n",
        "!ls s2v_old &> /dev/null\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "print(\"Installations Complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìö **Text File Formatting**\n",
        "When it comes to reading in files, you must ensure that the input files are of the correct format. \n",
        "The correct format for the best possible content generation should include:\n",
        "*   A .txt file with no unknown or broken characters\n",
        "*   No extraneous text (**anything not related to the main story**). Removing this stops any chance of these details being used in question generation. **Examples of extraneous details that can interfere with question generation include:**\n",
        " *   Page Numbers\n",
        " *   Publishing Details (including Author Name, ISBN)\n",
        " *   Standalone 1-2 Word Image Captions \n",
        " *   Question/activity sections that may potentially appear throughout, or at the end of the book \n",
        "\n",
        "\n",
        "Due to all books having different patterns when it comes to when and where these extraneous details may potentially appear, this process has to be done **manually**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "szmowxLxahV8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìë **Adding Text Files To Be Read In**\n",
        "All story book files to be processed must be uploaded as a zip folder known as \"stories.zip\".\n",
        "\n",
        "The stories.zip file must be in such a way that it can be read by a program. To convert an archive file with stories into a stories.zip, follow the steps within this program found [here](https://colab.research.google.com/drive/1gDq0Mh2Eqjt2sIaF8rqrDtFGg_HTqgYj?usp=sharing )\n",
        "\n",
        "To upload the zip file, navigate to the \"files\" section to the left of this browser, and click on \"upload to session storage\".\n",
        "[Click me](https://gyazo.com/726677814c4f254f3baf36ce65b5a676) for a visual guide. \n",
        "\n",
        "Then, simply click on the zip stories.zip file and it will be added to this run-time. \n",
        "After this, execute the cell below\n",
        "\n",
        "‚ùì **Troubleshooting:**\n",
        "* Ensure that stories.zip has no capital letters\n",
        "* If the currrent run-time runs out, the zip file must be reuploaded (the run-time only runs out if the computer is turned off, internet connection is lost, or the browser is exited)\n",
        "* Don't forget to execute the code block below\n"
      ],
      "metadata": {
        "id": "4dZUNte5dwcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!unzip /stories.zip &> /dev/null \n",
        "!unzip /content/stories.zip &> /dev/null "
      ],
      "metadata": {
        "id": "W6ajFQhIf4aW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚è≠ **Running The Program**\n",
        "Once this has been completed, run the code block below. Once again, please allow 2-3 minutes for relevant packages to download. \n",
        "You will know it worked as intended if it prints out the names of all story book titles successfully. It will display those names for 10 seconds, then conclude, meaning it is now ready for action. \n"
      ],
      "metadata": {
        "id": "L444AntHjB_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# relevant libraries\n",
        "from IPython.display import clear_output\n",
        "from Questgen import main\n",
        "from pprint import pprint\n",
        "import nltk.data\n",
        "import random \n",
        "import time\n",
        "import csv \n",
        "import os\n",
        "import re\n",
        "# for punctuation checking\n",
        "regex = re.compile('[,\\.!?]')\n",
        "\n",
        "# used for question generation\n",
        "qe = main.BoolQGen()\n",
        "answer = main.AnswerPredictor()\n",
        "qg = main.QGen()\n",
        "\n",
        "# test reading in of extracted files\n",
        "filenum = 0\n",
        "for filename in os.listdir(\"/content/stories\"):\n",
        "    if filename.endswith(\"txt\"): \n",
        "      filenum += 1\n",
        "      print(filename)\n",
        "\n",
        "time.sleep(10)\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "xMx3rE9Ge7pd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "178cd884-bf2e-4a3b-e54c-4850c99a989f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-265b0df18938>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#qe = main.BoolQGen()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#answer = main.AnswerPredictor()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mqg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQGen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# test reading in of extracted files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/Questgen/main.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT5Tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m't5-base'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT5ForConditionalGeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Parth/result'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0;31m# Instantiate model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstate_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfrom_tf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_t5.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_input_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36minit_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0;34m\"\"\" Initialize and prunes weights if needed. \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m         \u001b[0;31m# Initialize weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 450\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[0;31m# Prune heads if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    615\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 617\u001b[0;31m         \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_t5.py\u001b[0m in \u001b[0;36m_init_weights\u001b[0;34m(self, module)\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0;31m# Mesh TensorFlow embeddings initialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m             \u001b[0;31m# See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L1624\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfactor\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT5DenseReluDense\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m             \u001b[0;31m# Mesh TensorFlow FF initialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Next, run the below code block to generate questions!**\n",
        "As a general rule of thumb, it takes approximate 10 minutes of processing time per 100 stories, though of course this can vary depending on things like story length. During this processing time you can just have the program running in the background and do something else. The message \"PROCESSING COMPLETE\" will appear in the output box once the program is finished.\n"
      ],
      "metadata": {
        "id": "WQ9P-Z9OoMji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_progress(percent, barLen = 25): # this will print out current completion\n",
        "    # it is defined as the percentage of current files processed divided by total files available\n",
        "    progress = \"\"\n",
        "    for i in range(barLen):\n",
        "        if i < int(barLen * percent):\n",
        "            progress += \"‚ñà\"\n",
        "        else:\n",
        "            progress += \" \"\n",
        "    print(\"[%s] %.2f%%\" % (progress, percent * 100))\n",
        "              \n",
        "def scramble_words(text):\n",
        "  sorted_list = sorted(text.split(), key = len, reverse=True)\n",
        "  if len(sorted_list) <= 3:\n",
        "    longest_word = sorted_list[0]\n",
        "  else:\n",
        "    longest_word = random.choice(sorted_list[:3]) # retrieve one of the top 5 longest words in the text\n",
        "  count = 0\n",
        "  while \"-\" in longest_word or len(longest_word) <= 1:\n",
        "    longest_word = random.choice(sorted_list[:3])\n",
        "    count += 1\n",
        "    if count > 3:\n",
        "      return \"\"   \n",
        "  answer = regex.sub('', longest_word) # remove unnecessary punctuation\n",
        "  answer = re.sub(u'\\u2019', u\"\\u0027\", answer)\n",
        "  longest_word = answer\n",
        "  longest = list(longest_word)\n",
        "  # remove any punctuation marks\n",
        "  shuffled_word = ''.join([str(w) for w in random.sample(longest, len(longest))])\n",
        "  # turn it back to a string and convert to answer format\n",
        "  answer = (shuffled_word + \" | \" + answer + \" [CORRECT]\")\n",
        "  return answer\n",
        "\n",
        "def scramble_sentences(text):\n",
        "  sentences = nltk.tokenize.sent_tokenize(text) # tokenize the sentences\n",
        "  choice = random.choice(sentences) # choose a random sentence \n",
        "  random_sentence = (choice).split()\n",
        "  count = 0\n",
        "  while len(random_sentence) <= 2:\n",
        "    choice = random.choice(sentences) # choose a random sentence \n",
        "    random_sentence = (choice).split()\n",
        "    count += 1\n",
        "    if count >= 5:\n",
        "      return \"\"\n",
        "  random.shuffle(random_sentence) # rearrange random sentence order\n",
        "  shuffled_sentence = ' '.join(random_sentence) \n",
        "  answer = (shuffled_sentence + \" | \" + choice + \" [CORRECT]\") # convert to answer format\n",
        "  return answer\n",
        "\n",
        "def clean_word(random_word): # used to remove any trailing punctuation marks at the end (commas mainly)\n",
        "  original = random_word\n",
        "  random_word = list(random_word)\n",
        "  end = 0 \n",
        "  if not random_word[-1].isalpha():\n",
        "    end = str(random_word[-1])\n",
        "    random_word.remove(random_word[-1])\n",
        "  return ''.join(random_word), original, end\n",
        "\n",
        "def fill_blanks(text):\n",
        "  sentences = nltk.tokenize.sent_tokenize(text) # tokenize the sentences\n",
        "  random_sentence = (random.choice(sentences)).split() # choose a random sentence\n",
        "  copy = random_sentence[1:] # skip the first word\n",
        "  if len(copy) <= 1: \n",
        "    return \"\"\n",
        "  random_word = (random.choice(copy)) # choose a random word \n",
        "  random_word, original, end = clean_word((random_word)) # remove punctuation marks\n",
        "  count = 0\n",
        "  while len(random_word) < 3 and count <= 5: # make sure the word is long enough\n",
        "    random_word = (random.choice(random_sentence))\n",
        "    random_word,  original, end = clean_word((random_word))\n",
        "    count += 1\n",
        "    if count == 5: # if no words long enough found, skip the fill in the blanks qs\n",
        "      return \"\"\n",
        "  for item in range(len(random_sentence)): \n",
        "    new_word = random_sentence[item]\n",
        "    new_word, original, end = clean_word(new_word) \n",
        "    if new_word == random_word: \n",
        "      if end != 0: # replace word in original sentence with blanks (_)\n",
        "        input = \"_\"*(len(random_word))+str(end)\n",
        "        random_sentence[item] = input\n",
        "      else:\n",
        "        random_sentence[item] = \"_\"*(len(random_word))\n",
        "  blanked_sentence = ' '.join(random_sentence)\n",
        "  answer = (blanked_sentence + \" | \" + random_word + \" [CORRECT]\") # convert to answer format\n",
        "  return answer\n",
        "\n",
        "def create_mcq(payload):\n",
        "  output = qg.predict_mcq(payload)\n",
        "  question_storage = []\n",
        "  contexts = []\n",
        "  for key,value in output.items():\n",
        "    if key == 'questions':\n",
        "      for outputs in value:\n",
        "        for key2, value2 in outputs.items():\n",
        "            if key2 == 'question_statement': # this is the actual question\n",
        "                    question = value2\n",
        "            if key2 == 'answer': # this is the answer to the question\n",
        "                    answer = value2\n",
        "            if key2 == 'options': # these are the other (wrong) multiple choice answers\n",
        "                    options = \",\".join(value2)\n",
        "            if key2 == 'context': \n",
        "                    # these are contexts for each question: \n",
        "                    # the section of text from which the question was derived\n",
        "                    contexts.append(value2)\n",
        "        finalstring = question + \",\" + answer + \"[CORRECT],\" + options\n",
        "        question_storage.append(finalstring)\n",
        "  return question_storage, contexts\n",
        "\n",
        "# create output.csv document\n",
        "with open(\"/output.csv\", 'w') as f:\n",
        "  # these are the headings: can change as needed from here \n",
        "  header = ['Book Title', 'Sentence Scramble', 'Word Scramble', 'Fill Blanks', 'MCQ1', 'MCQ2', 'MCQ3'] #, 'Context1', 'Context2', 'Context3'\n",
        "  writer = csv.writer(f)\n",
        "  writer.writerow(header)\n",
        "  writer.writerow(\"\")\n",
        "\n",
        "testmode = True\n",
        "# When this is set to 'True', it will make a test document \n",
        "# This will only run a few documents at a time, and you can manually \n",
        "# prompt to exit - this can be used for just testing formatting and \n",
        "# making sure the program set up is working (if needed). \n",
        "# By default, this value is False for normal operation. \n",
        "count = 0\n",
        "iterations = 5 # change to how many stories you want to test out in a single run\n",
        "for filename in os.listdir(\"/content/stories\"):\n",
        "    count += 1\n",
        "    if count >= iterations and testmode:\n",
        "      break\n",
        "    if count % 5 == 0:\n",
        "      print(count)\n",
        "      clear_output()\n",
        "    if filename.endswith(\"txt\"): \n",
        "      with open(\"/content/stories/\" + filename, 'r') as file:\n",
        "          data = file.read().replace('\\n', ' ') # read in all story data as single string\n",
        "          data = re.sub(u'\\u2019',u\"\\u0027\", data)\n",
        "          print(\"Creating Questions For \" + filename)\n",
        "          payload = {\"input_text\": data}\n",
        "          all_qs = [] # used to store all question types\n",
        "          mcq_qs, contexts = create_mcq(payload) # for multiple choice qs\n",
        "          blanks = fill_blanks(data) # for fill in the blank qs\n",
        "          scrambled_sentences = scramble_sentences(str(data)) # for unscrambling full sentence qs\n",
        "          scrambled_words = scramble_words(str(data)) # for unscrambling word qs\n",
        "          print(\"Writing New Output To CSV\")\n",
        "          with open(\"/output.csv\", 'a', encoding='utf-16') as f: \n",
        "              all_qs.insert(0, filename[:-4]) # Adds the book name\n",
        "              # Adds the 3-5 generated questions\n",
        "              all_qs.append(scrambled_sentences) \n",
        "              all_qs.append(scrambled_words)\n",
        "              all_qs.append(blanks)\n",
        "              for item in mcq_qs:\n",
        "                all_qs.append(item)\n",
        "              # adds context for each mcq question on same row \n",
        "              fixrow = ['', '', '', '']\n",
        "              for item in contexts:\n",
        "                fixrow.append(item)\n",
        "              writer = csv.writer(f) \n",
        "              writer.writerow(all_qs)\n",
        "              writer.writerow(fixrow)\n",
        "          print(\"MODEL PROGRESS: \", end=' ')\n",
        "          if not testmode:\n",
        "            iterations = filenum\n",
        "          progress = float(count/iterations)\n",
        "          draw_progress(progress)\n",
        "\n",
        "# automatically downloads the csv to browser once complete\n",
        "print(\"PROCESSING COMPLETE, DOWNLOADING FILES\")\n",
        "from google.colab import files\n",
        "files.download('/output.csv')"
      ],
      "metadata": {
        "id": "jolwHDsbltjx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "outputId": "ddac5bdc-beb9-4bd4-ed97-ab08fc860a41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating Questions For The CD Book.txt\n",
            "Running model for generation\n",
            " Sense2vec_distractors successful for word :  carrot cake\n",
            " Sense2vec_distractors successful for word :  ducks\n",
            "Writing New Output To CSV\n",
            "MODEL PROGRESS:  [‚ñà‚ñà‚ñà‚ñà‚ñà                    ] 20.00%\n",
            "Creating Questions For Freddy the Frog Prince.txt\n",
            "Running model for generation\n",
            " Sense2vec_distractors successful for word :  freddy\n",
            " Sense2vec_distractors successful for word :  friday evening\n",
            "Writing New Output To CSV\n",
            "MODEL PROGRESS:  [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               ] 40.00%\n",
            "Creating Questions For Fables and Fairy Tales for Kids 3.txt\n",
            "Running model for generation\n",
            " Sense2vec_distractors successful for word :  gerda\n",
            " Sense2vec_distractors successful for word :  thumbelina\n",
            " Sense2vec_distractors successful for word :  kay\n",
            " Sense2vec_distractors successful for word :  snow white\n",
            "Writing New Output To CSV\n",
            "MODEL PROGRESS:  [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          ] 60.00%\n",
            "Creating Questions For The Kite.txt\n",
            "Running model for generation\n",
            " Sense2vec_distractors successful for word :  elisa\n",
            " Sense2vec_distractors successful for word :  ehud\n",
            " Sense2vec_distractors successful for word :  kite\n",
            "Writing New Output To CSV\n",
            "MODEL PROGRESS:  [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà     ] 80.00%\n",
            "PROCESSING COMPLETE, DOWNLOADING FILES\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_289f4dc8-d3f1-414c-a15e-25ebc228feb9\", \"output.csv\", 7425)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚úÖ **Finalisation**\n",
        "All questions generated will automatically be written to the output.csv folder, which will then automatically be downloaded.\n",
        "\n",
        "### ‚ùó **Some Important Final Notes**\n",
        "This program will not always generate the ideal questions, and may sometimes lack coherency due to current deficiencies in its language processing. **Therefore, some degree of manual checking will almost always be needed.** That being said, it can still serve as a solid foundation, as most questions only need a minimal form of human editing to be up to standard. \n",
        "\n",
        "### üì≠ **Further Contact Details** \n",
        "This program was made by Himath Ratnayake for MELearning. For anybody who may use/improve upon this program in the future,  please feel free to reach out if you have any further queries and questions. \n",
        "* üìß **E-Mail 1:** himath4510@gmail.com\n",
        "* üìß **E-Mail 2:** himath.ratnayake@griffithuni.edu.au\n",
        "* üìß **Discord ID:** <@239156955988492298> (post this on any discord server to access profile)\n",
        "\n",
        "### ‚≠ê **Other Useful Resources**\n",
        "* **QuestGen Open-Source Library:** \n",
        " The primary tool used to generate the multiple choice questions, using a module that has already been carefully developed and trained. \n",
        " * Available at: https://github.com/ramsrigouthamg/Questgen.ai \n",
        "\n",
        "\n",
        "* **Natural Language Toolkit and Corpora:** \n",
        "For relevant lexical resources (word banks), text processing features (tokenization) and other NLP tasks. \n",
        " * Available at: https://www.nltk.org/ \n",
        "\n",
        "* **File Processing Program Link:**\n",
        " * Available at: https://colab.research.google.com/drive/1gDq0Mh2Eqjt2sIaF8rqrDtFGg_HTqgYj?usp=sharing \n",
        "\n"
      ],
      "metadata": {
        "id": "MZEA8Q44pLxd"
      }
    }
  ]
}